{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"2j3PRu0yKT1O","outputId":"3c6073e4-f55e-46c9-9cf3-edf6b66458e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","cluster-38ae  GCE       2                                             RUNNING  us-central1-a\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["!gcloud dataproc clusters list --region us-central1\n","!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","import pyspark\n","import math\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from nltk.stem import PorterStemmer\n","stemmer = PorterStemmer()\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","import builtins\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from collections import Counter\n","import re\n","from math import log\n","\n","\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import sys\n","from collections import Counter, OrderedDict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","from time import time\n","from pathlib import Path\n","import pickle\n","from google.cloud import storage\n","from collections import defaultdict\n","from contextlib import closing\n","\n","PROJECT_ID = 'triple-bird-414408'\n","\n","def get_bucket(bucket_name):\n","    return storage.Client(PROJECT_ID).bucket(bucket_name)\n","\n","def _open(path, mode, bucket=None):\n","    if bucket is None:\n","        return open(path, mode)\n","    return bucket.blob(path).open(mode)\n","\n","\n","TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this \n","                     # many bytes.\n","TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"JQysfflGKb8H","outputId":"720cbdca-4380-413d-bec1-bbfc01162d28"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index_gcp.py\n","# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"vgI4ZBmeXpTY","outputId":"ce190acd-7688-46d6-f85e-243dd9a8d137"},"outputs":[],"source":["# %cd -q /home/dataproc\n","# !ls search_backend.py\n","# # adding our python module to the cluster\n","# sc.addFile(\"/home/dataproc/search_backend.py\")\n","# sys.path.insert(0,SparkFiles.getRootDirectory())\n","# # from search_backend import *"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8FED5HNEKdrV"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#load the paths of the preprocessed data from the bucket\n","bucket_name = '204800122_wikidata'\n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and b.name != 'init_file.sh':\n","        paths.append(full_path+b.name)\n","        \n","#Take only 1 file for testing\n","paths_2 = paths[:1]\n","corpus = spark.read.parquet(*paths_2)\n","corpus_len = corpus.count()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Update your stopwords setup as before\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = set([word.lower() for word in english_stopwords.union(corpus_stopwords)])\n","\n","ps = PorterStemmer()\n","\n","token_pattern = re.compile(r\"\"\"[\\w](['\\-]?\\w)*\"\"\")\n","\n","def get_tokens(text, stem=False, bigrams=False, trigrams=False):\n","    tokens = word_tokenize(text.lower())\n","    filtered_tokens = [token for token in tokens if token not in all_stopwords and token_pattern.match(token)]\n","    \n","    # Apply stemming if requested\n","    if stem:\n","        filtered_tokens = [ps.stem(token) for token in filtered_tokens]\n","    \n","    # Generate bigrams if requested\n","    if bigrams:\n","        filtered_tokens = [f\"{filtered_tokens[i]} {filtered_tokens[i+1]}\" for i in range(len(filtered_tokens)-1)]\n","        \n","    # Generate trigrams if requested\n","    if trigrams:\n","        # This will create a list of trigrams where each trigram is represented as a single string\n","        filtered_tokens = [f\"{filtered_tokens[i]} {filtered_tokens[i+1]} {filtered_tokens[i+2]}\" for i in range(len(filtered_tokens)-2)]\n","    \n","    return filtered_tokens\n","\n","def word_count(text, id,stem=False,bigrams=False, trigrams=False):\n","    # Use get_tokens to process the text\n","    tokens = get_tokens(text,stem,bigrams,trigrams)\n","    return [(term, (id, tf)) for term, tf in Counter(tokens).items()]\n","\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def calculate_df(postings):\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","\n","def reduce_word_counts(unsorted_pl):\n","    return sorted(unsorted_pl, key=lambda x: x[0], reverse=True)\n","\n","\n","def partition_postings_and_write(postings, bucket_name):\n","    print(bucket_name)\n","    return postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1]))).groupByKey().mapValues(list).map(lambda x: InvertedIndex.write_a_posting_list(b_w_pl=x, base_dir='', bucket_name=bucket_name))\n","\n","def calc_tdidf(text, id, df, corpus_size,stem=False,bigrams=False, trigrams=False):\n","    \"\"\"without stemming\"\"\"\n","    tokens = get_tokens(text,stem,bigrams,trigrams)\n","    words_counter = Counter(tokens)\n","    size = builtins.sum([(c/len(tokens)*math.log2(corpus_size/df[w]))**2 for w,c in words_counter.items() if w in df])\n","    return (id, math.sqrt(size))\n","\n","\n","def anchors_to_pairs(row):\n","  pairs =[]\n","  for doc_id, anchor in row[0]:\n","    pairs.append((doc_id, anchor))\n","  return pairs"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def create_index(index_name,corpus_len,document_part,corpus,stem=False,bigrams=False,trigrams=False):\n","    index_name = f\"{index_name}\"        \n","    doc_pairs = corpus.select(document_part, \"id\").rdd\n","    word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1],stem,bigrams,trigrams))\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","    term_total = postings_filtered.map(lambda x: (x[0],builtins.sum([i[1] for i in x[1]])))\n","    doc_len = doc_pairs.map(lambda x: (x[1], len(get_tokens(x[0],stem,bigrams,trigrams))))\n","    w2df = calculate_df(postings_filtered)\n","    w2df_dict = w2df.collectAsMap()\n","    _ = partition_postings_and_write(postings_filtered, bucket_name).collect()\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix=''):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs[k].extend(v)\n","    # Create inverted index instance\n","    inverted = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted.df = w2df.collectAsMap()\n","    # Add the Document leangth -> use for bm25\n","    inverted.document_len = doc_len.collectAsMap()\n","    # Add the term_total\n","    inverted.term_total = term_total.collectAsMap()\n","    #calculate tdidf\n","    tf_score = doc_pairs.map(lambda x: calc_tdidf(x[0], x[1], w2df_dict, corpus_len,stem,bigrams,trigrams))\n","    inverted.document_size = dict(tf_score.collect())\n","    # write the global stats out\n","    inverted.write_index('.', f'index_{index_name}')\n","\n","    # upload to gs\n","    index_src = f\"index_{index_name}.pkl\"\n","    index_dst = f'gs://{bucket_name}/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","\n","    print(index_src)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["bucket_name = \"anchor_index_nostem\"\n","doc_anchor = corpus.select(\"anchor_text\").rdd\n","inverted_anchor = InvertedIndex()\n","doc_anchor_pairs = doc_anchor.flatMap(anchors_to_pairs).groupByKey().mapValues(list).map(lambda x: (\" \".join(x[1]), x[0]))\n","word_counts = doc_anchor_pairs.flatMap(lambda x: word_count(x[0], x[1],stem=False,bigrams=False,trigrams=False))\n","doc_len = doc_anchor_pairs.map(lambda x: (x[1], len(get_tokens(x[0],stem=False,bigrams=False,trigrams=False))))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["len_docs_anchor = doc_len.collectAsMap()\n","inverted_anchor.document_len = len_docs_anchor"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["from operator import add\n","postings_anchor = word_counts.groupByKey().mapValues(reduce_word_counts)\n","total_terms_anchor = postings_anchor.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add)\n","inverted_anchor.term_total = total_terms_anchor.collectAsMap()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["w2df_anchor = calculate_df(postings_anchor)\n","w2df_anchor_dict = w2df_anchor.collectAsMap()\n","inverted_anchor.df = w2df_anchor_dict"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["anchor_index_nostem\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["_ = partition_postings_and_write(postings_anchor,\"anchor_index_nostem\").collect()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# tf_score = doc_anchor_pairs.map(lambda x: calc_tdidf(x[0], x[1], w2df_anchor_dict, corpus_len,stem=False,bigrams=False,trigrams=False))\n","# inverted_anchor.document_size = dict(tf_score.collect())"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://index_anchor_nostem.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 43.2 MiB/ 43.2 MiB]                                                \n","Operation completed over 1 objects/43.2 MiB.                                     \n"]},{"name":"stderr","output_type":"stream","text":["24/02/26 17:34:00 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_13_python !\n"]}],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_anchor = defaultdict(list)\n","for blob in client.list_blobs(bucket_name):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs_anchor[k].extend(v)\n","\n","#stores it in anchor_index.posting_locs attribute\n","inverted_anchor.posting_locs = super_posting_locs_anchor\n","inverted_anchor.write_index('.','index_anchor_nostem')\n","\n","# upload to gs\n","index_src = \"index_anchor_nostem.pkl\"\n","index_dst = f'gs://anchor_index_nostem/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# def anchor_index_creation(index_name,corpus_len,corpus):\n","#     index_name = f'{index_name}'   \n","#     # Convert the corpus DataFrame into an RDD of anchor text and document ID pairs\n","#     anchor_pairs = corpus.select(\"anchor_text\", \"id\").rdd\n","    \n","#     # Manipulate the tuples so each tuple contains (text, the doc_id of the document we found the anchor_text)\n","#     postings_not_grouped = anchor_pairs.flatMap(lambda row: [(anchor.text, row.id) for anchor in row.anchor_text])\n","#     doc_len = doc_pairs.map(lambda x: (x[1], len(get_tokens(x[0],stem,bigrams,trigrams))))\n","#     postings = anchor_pairs.flatMap(lambda row: [(anchor.text, row.id) for anchor in row.anchor_text]).groupByKey()\n","#     anchor_posting = postings.mapValues(list).map(lambda x: (x[0], Counter(x[1]).most_common()))\n","    \n","#     # Filter postings to include only those with more than 0 occurrences\n","#     postings_filtered = anchor_posting.filter(lambda x: len(x[1]) > 0)  # (\"word\", {doc_id1: number of appearances})\n","    \n","#     # Create an inverted index instance\n","#     anchor_InvertedIndex = InvertedIndex()\n","    \n","#     # Calculate document frequency and update the inverted index\n","#     anchor_InvertedIndex.df = dict(Counter(calculate_df(postings_filtered).collectAsMap()))\n","    \n","#     # Partition postings and write them to the specified bucket\n","#     _ = partition_postings_and_write(postings_filtered, bucket_name).collect()\n","    \n","#     # Collect all posting lists locations into one super-set\n","#     super_posting_locs_text = defaultdict(list)\n","#     for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","#         if not blob.name.endswith(\"pickle\"):\n","#             continue\n","#         with blob.open(\"rb\") as f:\n","#             posting_locs = pickle.load(f)\n","#             for k, v in posting_locs.items():\n","#                 super_posting_locs_text[k].extend(v)\n","                \n","#     # Update the inverted index with postings locations\n","#     anchor_InvertedIndex.posting_locs = super_posting_locs_text\n","#     anchor_InvertedIndex.document_len = doc_len.collectAsMap()\n","#     # Add the term_total\n","#     anchor_InvertedIndex.term_total = term_total.collectAsMap()\n","#     #calculate tdidf\n","#     tf_score = postings_not_grouped.map(lambda x: calc_tdidf(x[0], x[1], w2df_dict, corpus_len,stem,bigrams,trigrams))\n","#     anchor_InvertedIndex.document_size = dict(tf_score.collect())\n","#     anchor_InvertedIndex.write_index('.', f'index_{index_name}')\n","\n","#     # upload to gs\n","#     index_src = f\"index_{index_name}.pkl\"\n","#     index_dst = f'gs://{bucket_name}/{index_src}'\n","#     !gsutil cp $index_src $index_dst\n"]},{"cell_type":"code","execution_count":60,"metadata":{"scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["title_index_nostem\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://index_title_index_withStem_bigrams.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][  3.4 MiB/  3.4 MiB]                                                \n","Operation completed over 1 objects/3.4 MiB.                                      \n","index_title_index_withStem_bigrams.pkl\n"]}],"source":["bucket_name = \"title_index_nostem\"\n","\n","index_location = create_index(\"title_index_withStem_bigrams\", corpus_len, \"title\", corpus, stem=False, bigrams=False, trigrams=False)"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["text_index_nostem\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Copying file://index_text_index_nostem.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][  5.8 MiB/  5.8 MiB]                                                \n","Operation completed over 1 objects/5.8 MiB.                                      \n","index_text_index_nostem.pkl\n"]}],"source":["bucket_name = \"text_index_nostem\"\n","index_location = create_index(\"text_index_nostem\", corpus_len, \"text\", corpus, stem=False, bigrams=False, trigrams=False)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pairs = corpus.select(\"id\", \"title\").rdd\n","\n","# Convert RDD to a Python dictionary\n","id_to_title_dict = dict(pairs.collect())\n","\n","# Specify the path where you want to save the pickle file\n","pickle_file_name = \"id_to_title_dict.pkl\"\n","bucket_name = \"title_index_nostem\"\n","client = storage.Client()\n","bucket = client.bucket(bucket_name)\n","blob = bucket.blob(pickle_file_name)\n","pickle_bytes = pickle.dumps(id_to_title_dict)\n","blob.upload_from_string(pickle_bytes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}